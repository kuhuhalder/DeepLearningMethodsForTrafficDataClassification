{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inter-Arrival Time (in secs)</th>\n",
       "      <th>Packet Size</th>\n",
       "      <th>MCS</th>\n",
       "      <th>NB_RB</th>\n",
       "      <th>Application Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008782</td>\n",
       "      <td>802</td>\n",
       "      <td>19.86</td>\n",
       "      <td>9.65</td>\n",
       "      <td>VIDEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>802</td>\n",
       "      <td>18.99</td>\n",
       "      <td>11.15</td>\n",
       "      <td>PERF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013127</td>\n",
       "      <td>802</td>\n",
       "      <td>19.86</td>\n",
       "      <td>9.65</td>\n",
       "      <td>VIDEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>802</td>\n",
       "      <td>18.99</td>\n",
       "      <td>11.15</td>\n",
       "      <td>PERF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>802</td>\n",
       "      <td>18.99</td>\n",
       "      <td>11.15</td>\n",
       "      <td>PERF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inter-Arrival Time (in secs)  Packet Size    MCS  NB_RB  \\\n",
       "0                      0.008782          802  19.86   9.65   \n",
       "1                      0.000002          802  18.99  11.15   \n",
       "2                      0.013127          802  19.86   9.65   \n",
       "3                      0.000002          802  18.99  11.15   \n",
       "4                      0.000002          802  18.99  11.15   \n",
       "\n",
       "  Application Category   \n",
       "0                 VIDEO  \n",
       "1                  PERF  \n",
       "2                 VIDEO  \n",
       "3                  PERF  \n",
       "4                  PERF  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Final Randomized Dataset v2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13080\n",
      "5606\n",
      "13080\n",
      "5606\n"
     ]
    }
   ],
   "source": [
    "X=data[['Inter-Arrival Time (in secs)','Packet Size','MCS', 'NB_RB']]\n",
    "Y=data['Application Category ']\n",
    "#X_train, X_test, Y_train, Y_test=train_test_split(X, Y, train_size=0.7, random_state=42)\n",
    "#print(len(X_train))\n",
    "#print(len(X_test))\n",
    "#print(len(Y_train))\n",
    "#print(len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = LabelEncoder()\n",
    "#encoder= data.apply(LabelEncoder().fit_transform)\n",
    "#encoder.fit(Y_train)\n",
    "\n",
    "#encoder=OneHotEncoder().fit_transform(Y_train)\n",
    "#encoded_Y = encoder.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoder)\n",
    "#dummy_y\n",
    "\n",
    "x_vars_stdscle=StandardScaler().fit_transform(X) # standardizing the X \n",
    "X_train, X_test, Y_train, Y_test= train_test_split(x_vars_stdscle, Y, train_size=0.7, random_state=42)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# one hot encoded\n",
    "Y_encoded = np_utils.to_categorical(encoded_Y)\n",
    "Y_encoded[:3]\n",
    "\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "Y_train_encoded=np_utils.to_categorical(encoded_Y_train)\n",
    "Y_train_encoded\n",
    "\n",
    "encoder.fit(Y_test)\n",
    "encoded_Y_test=encoder.transform(Y_test)\n",
    "Y_test_encoded=np_utils.to_categorical(encoded_Y_train)\n",
    "Y_test_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61307931, 0.61318634])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read about the different kinds of optimizers\n",
    "def create_network():\n",
    "    network=models.Sequential()\n",
    "    network.add(layers.Dense(units=2, activation='relu', input_shape=(4,) ))\n",
    "    network.add(layers.Dense(units=2, activation ='relu'))\n",
    "    network.add(layers.Dense(units=4, activation='softmax'))\n",
    "    network.compile(loss='categorical_crossentropy', optimizer ='rmsprop', metrics=['accuracy'])\n",
    "    return network\n",
    "\n",
    "neural_network=KerasClassifier(build_fn=create_network, epochs=10, batch_size=100, verbose=0)# debugging turned off by setting verbose to 0\n",
    "seed=7\n",
    "#np.random.seed(seed)\n",
    "#kfold=KFold(n_splits=10, shuffle=True)#, random_state=seed)\n",
    "cross_val_score(neural_network, X, Y_encoded, cv=2)\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "### !!!!!!OVERFITTING PROBLEMMM!!!!!\n",
    "## Graph it to see the change of cross validation score with the epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for tuning the parameters of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-de39dac4fc68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgrid_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_network(optimizer='rmsprop'):\n",
    "    network=models.Sequential()\n",
    "    network.add(layers.Dense(units=2, activation='relu', input_shape=(4,) ))\n",
    "    network.add(layers.Dense(units=2, activation ='relu'))\n",
    "    network.add(layers.Dense(units=4, activation='softmax'))\n",
    "    network.compile(loss='categorical_crossentropy', optimizer =optimizer, metrics=['accuracy'])\n",
    "    return network\n",
    "\n",
    "neural_network=KerasClassifier(build_fn=create_network, verbose=0)# debugging turned off by setting verb\n",
    "epochs=[5,10,15,20]\n",
    "batches=[5,10,100]\n",
    "optimizers=['rmsprop', 'adam']\n",
    "hyperparameters=dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "grid=GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "grid_result=grid.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=models.Sequential()\n",
    "network.add(layers.Dense(units=3, activation='relu', input_shape=(4,) ))\n",
    "network.add(layers.Dense(units=3, activation ='relu'))\n",
    "network.add(layers.Dense(units=4, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer ='rmsprop', metrics=['accuracy'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12980 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "12980/12980 [==============================] - 2s 185us/step - loss: 0.4748 - val_loss: 0.1546\n",
      "Epoch 2/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4774 - val_loss: 0.1566\n",
      "Epoch 3/10\n",
      "12980/12980 [==============================] - 0s 21us/step - loss: 0.4802 - val_loss: 0.1543\n",
      "Epoch 4/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4785 - val_loss: 0.1555\n",
      "Epoch 5/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4842 - val_loss: 0.1553\n",
      "Epoch 6/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4654 - val_loss: 0.1530\n",
      "Epoch 7/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4821 - val_loss: 0.1544\n",
      "Epoch 8/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4853 - val_loss: 0.1542\n",
      "Epoch 9/10\n",
      "12980/12980 [==============================] - 0s 21us/step - loss: 0.4847 - val_loss: 0.1570\n",
      "Epoch 10/10\n",
      "12980/12980 [==============================] - 0s 20us/step - loss: 0.4797 - val_loss: 0.1535\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X24VXWd9/H3h8OTPCPQ+IAImlMCIuCO6NbEpxzMBDNLUEtNh3Q0Leu+ZdTKKK8x81ajYUxrpCZQxnQsMpWppMy7VA5mGBgDIuoJ0iMKgiB4Dt/7j7XOYZ/DPmedp332Bj6v61rXXuu3fmut71577/Xd6+m3FBGYmZk1p0upAzAzs/LnZGFmZpmcLMzMLJOThZmZZXKyMDOzTE4WZmaWycnCOoWkCklbJA3ryLqlJOm9kjr82nNJp0hamze8UtKHW1K3Dcv6gaRr2zp9M/P9pqQfdvR8rXS6ljoAK0+StuQN9gK2A7Xp8OciYn5r5hcRtUCfjq67L4iI93XEfCRdApwfESfkzfuSjpi37f2cLKygiKjfWKf/XC+JiF81VV9S14io6YzYzKzz+TCUtUl6mOE/Jd0raTNwvqQPSXpS0kZJ6yXNltQtrd9VUkgang7PS8c/ImmzpD9IGtHauun40yT9j6RNkr4r6f9JurCJuFsS4+ckrZb0pqTZedNWSLpN0gZJLwCTm1k/10ta0KhsjqRb0/5LJD2fvp8X0n/9Tc2rStIJaX8vST9OY1sOHFNguWvS+S6XNCUtPwr4V+DD6SG+1/PW7Q1501+avvcNkn4q6cCWrJssks5M49ko6TFJ78sbd62kdZLekvSXvPc6UdIzafmrkr7d0uVZEUSEO3fNdsBa4JRGZd8EdgBnkPzp2A/4APBBkj3Ww4D/Aa5I63cFAhieDs8DXgdyQDfgP4F5baj7HmAzMDUddzXwLnBhE++lJTH+DOgPDAfeqHvvwBXAcmAoMAh4PPkJFVzOYcAWoHfevF8DcunwGWkdAScB24Ax6bhTgLV586oCTkj7bwF+AwwEDgVWNKr7KeDA9DM5N43h79JxlwC/aRTnPOCGtP/UNMaxQE/g34DHWrJuCrz/bwI/TPuPTOM4Kf2Mrk3XezdgFPAScEBadwRwWNq/BJie9vcFPljq38K+3HnPwtrjiYj4eUTsjIhtEbEkIp6KiJqIWAPcBUxqZvr7I6IyIt4F5pNspFpb92PAsxHxs3TcbSSJpaAWxvgvEbEpItaSbJjrlvUp4LaIqIqIDcBNzSxnDfBnkiQG8BFgY0RUpuN/HhFrIvEY8Gug4EnsRj4FfDMi3oyIl0j2FvKXe19ErE8/k3tIEn2uBfMFOA/4QUQ8GxHvADOBSZKG5tVpat00ZxqwMCIeSz+jm4B+JEm7hiQxjUoPZb6YrjtIkv4RkgZFxOaIeKqF78OKwMnC2uOV/AFJ75f0C0l/k/QWMAsY3Mz0f8vr30rzJ7WbqntQfhwREST/xAtqYYwtWhbJP+Lm3ANMT/vPJUlydXF8TNJTkt6QtJHkX31z66rOgc3FIOlCSX9KD/dsBN7fwvlC8v7q5xcRbwFvAgfn1WnNZ9bUfHeSfEYHR8RK4Eskn8Nr6WHNA9KqFwEjgZWSnpb00Ra+DysCJwtrj8aXjd5J8m/6vRHRD/gqyWGWYlpPclgIAEmi4catsfbEuB44JG8469Le/wROSf+ZTyVJHkjaD7gf+BeSQ0QDgP9uYRx/ayoGSYcBdwCXAYPS+f4lb75Zl/muIzm0VTe/viSHu/7agrhaM98uJJ/ZXwEiYl5EHEtyCKqCZL0QESsjYhrJocb/CzwgqWc7Y7E2crKwjtQX2AS8LelI4HOdsMyHgPGSzpDUFbgKGFKkGO8DviDpYEmDgGuaqxwRrwJPAHOBlRGxKh3VA+gOVAO1kj4GnNyKGK6VNEDJfShX5I3rQ5IQqkny5iUkexZ1XgWG1p3QL+Be4GJJYyT1INlo/y4imtxTa0XMUySdkC77f5OcZ3pK0pGSTkyXty3taknewKclDU73RDal721nO2OxNnKysI70JeACkg3BnST/rIsq3SCfA9wKbAAOB/5Icl9IR8d4B8m5hedITr7e34Jp7iE5YX1PXswbgS8CD5KcJD6bJOm1xNdI9nDWAo8A/5E332XAbODptM77gfzj/L8EVgGvSso/nFQ3/aMkh4MeTKcfRnIeo10iYjnJOr+DJJFNBqak5y96ADeTnGf6G8mezPXppB8Fnldytd0twDkRsaO98VjbKDnEa7Z3kFRBctjj7Ij4XanjMdtbeM/C9niSJkvqnx7K+ArJFTZPlzgss72Kk4XtDY4D1pAcypgMnBkRTR2GMrM2KGqySP/xrUzv+JxZYPyFkqolPZt2l+SNu0DSqrS7oJhx2p4tIq6PiEER0TciJkbEklLHZLa3Kdo5i/TY8f+Q3IxUxa67MVfk1bmQ5I7WKxpNuz9QSXIzUQBLgWMi4s2iBGtmZs0qZkOCE4DVdXdjpu3kTCVpniDLPwC/jIg30ml/SXJ44d6mJhg8eHAMHz68vTGbme1Tli5d+npENHe5OVDcZHEwDe80rSK5vb+xT0g6nmQv5IsR8UoT0+52o5WkGcAMgGHDhlFZWdlBoZuZ7RskZbVEABT3nEWhu1EbH/P6OUljcWOAXwE/asW0RMRdEZGLiNyQIZmJ0czM2qiYyaKKhs0SDCW5/r1eRGzIu2rl++xqbjlzWjMz6zzFTBZLSFqMHCGpO2nLk/kV6trKT00Bnk/7FwGnShooaSBJI2uLihirmZk1o2jnLCKiRtIVJBv5CuDuiFguaRZQGRELgSvTh7PUkDR7cGE67RuSvkGScABm1Z3sNjOzzrfXNPeRy+XCJ7jNzFpH0tKIyHzmie/gNrM9xvz5MHw4dOmSvM6fnzWFdRQnC7MmeMNUXubPhxkz4KWXICJ5nTGjNJ/LvvjdcLIwK6CcNkzlotQbyOuug61bG5Zt3ZqUd6Z99bvhZGH1Sr0xKCflsmEqF+WwgXz55daVF8u++t1wsjCgPDYG+bGUOmmVy4apXJTDBnJYEw+xbaq8WMrpu9Gpv5WI2Cu6Y445JvZU8+ZFHHpohJS8zpvX+TEcemhEkiYadoce2rlxzJsX0atXwxh69er8dVIu66NcSIXXh9R5Mfi70VBHrQ+SWxkyt7El38h3VNfWZFHqDXW5/ADKYWMQsff9EPcW5fS5lPqPVbl8NzrqM3GyaIFy+NDL5UdYLnGUS9KKKI8NU7koh99KOSmH70ZH/VacLFqgHDaQ5bJxLJeNQTl8JlZYOWwgbZfO3rPYp09wl8OJqnI5aXfeeXDXXXDooSAlr3fdlZR3phtvhF69Gpb16pWUW2mddx6sXQs7dyavnf3dsIY6+7eyTyeLcthQl9PGsRw2BuWStMzKXWf/VvbptqHqLhfNvySwV6/O3zjNn59cgvjyy0miuvFGbxzNrHO4bagWKJd/seXwj97KVzncd2JWzMeq7hHOO88bZytfjfd+626WBH9vrXPt03sWZuWuHO6cNgMnC7OyVg5X7JmBk4VZWSuHK/bMwMnCrKyV06XVtm9zsjArY+VyxZ7ZPn81lFm58xV7Vg68Z2FmZpmKmiwkTZa0UtJqSTObqXe2pJCUS4eHS9om6dm0+14x4zQzs+YV7TCUpApgDvARoApYImlhRKxoVK8vcCXwVKNZvBARY4sVn5mZtVwx9ywmAKsjYk1E7AAWAFML1PsGcDPwThFjMTOzdihmsjgYeCVvuCotqydpHHBIRDxUYPoRkv4o6beSPlxoAZJmSKqUVFldXd1hgZuZWUPFTBYqUFbfxK2kLsBtwJcK1FsPDIuIccDVwD2S+u02s4i7IiIXEbkhQ4Z0UNhmZtZYMZNFFXBI3vBQYF3ecF9gNPAbSWuBicBCSbmI2B4RGwAiYinwAvD3RYzVzMyaUcxksQQ4QtIISd2BacDCupERsSkiBkfE8IgYDjwJTImISklD0hPkSDoMOAJYU8RYzcysGUW7GioiaiRdASwCKoC7I2K5pFkkz3xd2MzkxwOzJNUAtcClEfFGsWI1M7Pm7dNPyjMz29f5SXlmZtZhnCzMzCyTk4WZmWVysjAzs0xOFmZmlsnJwszMMjlZmJlZJicLMzPL5GRhZmaZnCzMzCyTk4WZmWVysjAzs0xOFmZmlsnJwszMMjlZmJlZJicLMzPL5GRhZmaZnCzMzCyTk4WZmWVysjAzs0xOFmZmlqmoyULSZEkrJa2WNLOZemdLCkm5vLJ/TqdbKekfihmnmZk1r2uxZiypApgDfASoApZIWhgRKxrV6wtcCTyVVzYSmAaMAg4CfiXp7yOitljxmplZ04q5ZzEBWB0RayJiB7AAmFqg3jeAm4F38sqmAgsiYntEvAisTudnZmYlUMxkcTDwSt5wVVpWT9I44JCIeKi106bTz5BUKamyurq6Y6I2M7PdFDNZqEBZ1I+UugC3AV9q7bT1BRF3RUQuInJDhgxpc6BmZta8op2zINkbOCRveCiwLm+4LzAa+I0kgAOAhZKmtGBaMzPrRMXcs1gCHCFphKTuJCesF9aNjIhNETE4IoZHxHDgSWBKRFSm9aZJ6iFpBHAE8HQRYzUzs2YUbc8iImokXQEsAiqAuyNiuaRZQGVELGxm2uWS7gNWADXA5b4SysysdBSx26mAPVIul4vKyspSh2FmtkeRtDQicln1fAe3mZllcrIwM7NMThZmZpbJycLMzDI5WZiZWSYnCzMzy+RkYWZmmZwszMwsk5OFmZllcrIwM7NMThZmZpbJycLMzDI5WZiZWSYnCzMzy+RkYWZmmZwszMwsk5OFmZllcrIwM7NMThZmZpbJycLMzDI5WZiZWaaiJgtJkyWtlLRa0swC4y+V9JykZyU9IWlkWj5c0ra0/FlJ3ytmnGZm1ryuxZqxpApgDvARoApYImlhRKzIq3ZPRHwvrT8FuBWYnI57ISLGFis+MzNruWLuWUwAVkfEmojYASwApuZXiIi38gZ7A1HEeMzMrI2KmSwOBl7JG65KyxqQdLmkF4CbgSvzRo2Q9EdJv5X04UILkDRDUqWkyurq6o6M3czM8hQzWahA2W57DhExJyIOB64Brk+L1wPDImIccDVwj6R+Baa9KyJyEZEbMmRIB4ZuZmb5inbOgmRP4pC84aHAumbqLwDuAIiI7cD2tH9puufx90BlcUI1s9Z69913qaqq4p133il1KNYCPXv2ZOjQoXTr1q1N0xczWSwBjpA0AvgrMA04N7+CpCMiYlU6eDqwKi0fArwREbWSDgOOANYUMVYza6Wqqir69u3L8OHDkQodSLByERFs2LCBqqoqRowY0aZ5FC1ZRESNpCuARUAFcHdELJc0C6iMiIXAFZJOAd4F3gQuSCc/HpglqQaoBS6NiDeKFauZtd4777zjRLGHkMSgQYNoz7ndYu5ZEBEPAw83KvtqXv9VTUz3APBAMWMzs/ZzothztPez8h3cZrZH2rBhA2PHjmXs2LEccMABHHzwwfXDO3bsaNE8LrroIlauXNlsnTlz5jB//vyOCJnjjjuOZ599tkPm1dmKumdhZlZn/ny47jp4+WUYNgxuvBHOO6/t8xs0aFD9hveGG26gT58+fPnLX25QJyKICLp0Kfy/eO7cuZnLufzyy9se5F7EexZmVnTz58OMGfDSSxCRvM6YkZR3tNWrVzN69GguvfRSxo8fz/r165kxYwa5XI5Ro0Yxa9as+rp1//RramoYMGAAM2fO5Oijj+ZDH/oQr732GgDXX389t99+e339mTNnMmHCBN73vvfx+9//HoC3336bT3ziExx99NFMnz6dXC6XuQcxb948jjrqKEaPHs21114LQE1NDZ/+9Kfry2fPng3AbbfdxsiRIzn66KM5//zzO3ydtYT3LMys6K67DrZubVi2dWtS3p69i6asWLGCuXPn8r3vJc3K3XTTTey///7U1NRw4okncvbZZzNy5MgG02zatIlJkyZx0003cfXVV3P33Xczc+ZuTdoRETz99NMsXLiQWbNm8eijj/Ld736XAw44gAceeIA//elPjB8/vtn4qqqquP7666msrKR///6ccsopPPTQQwwZMoTXX3+d5557DoCNGzcCcPPNN/PSSy/RvXv3+rLO1qI9C0mHS+qR9p8g6UpJA4obmpntLV5+uXXl7XX44YfzgQ98oH743nvvZfz48YwfP57nn3+eFStW7DbNfvvtx2mnnQbAMcccw9q1awvO+6yzztqtzhNPPMG0adMAOProoxk1alSz8T311FOcdNJJDB48mG7dunHuuefy+OOP8973vpeVK1dy1VVXsWjRIvr37w/AqFGjOP/885k/f36b75Nor5YehnoAqJX0XuDfgRHAPUWLysz2KsOGta68vXr37l3fv2rVKr7zne/w2GOPsWzZMiZPnlzwRsLu3bvX91dUVFBTU1Nw3j169NitTkTrmrVrqv6gQYNYtmwZxx13HLNnz+Zzn/scAIsWLeLSSy/l6aefJpfLUVtb26rldYSWJoudEVEDfBy4PSK+CBxYvLDMbG9y443Qq1fDsl69kvJie+utt+jbty/9+vVj/fr1LFq0qMOXcdxxx3HfffcB8NxzzxXcc8k3ceJEFi9ezIYNG6ipqWHBggVMmjSJ6upqIoJPfvKTfP3rX+eZZ56htraWqqoqTjrpJL797W9TXV3N1sbH9DpBS89ZvCtpOslNc2ekZaXZFzKzPU7deYmOvBqqpcaPH8/IkSMZPXo0hx12GMcee2yHL+Pzn/88n/nMZxgzZgzjx49n9OjR9YeQChk6dCizZs3ihBNOICI444wzOP3003nmmWe4+OKLiQgk8a1vfYuamhrOPfdcNm/ezM6dO7nmmmvo27dvh7+HLGrJ7lP6UKJLgT9ExL1pEx7nRMRNxQ6wpXK5XFRWuukos87y/PPPc+SRR5Y6jLJQU1NDTU0NPXv2ZNWqVZx66qmsWrWKrl3L6xqiQp+ZpKURkcuatkXvJH1g0ZXpjAcCfcspUZiZldKWLVs4+eSTqampISK48847yy5RtFeL3o2k3wBT0vrPAtWSfhsRVxcxNjOzPcKAAQNYunRpqcMoqpae4O6fPtXuLGBuRBwDnFK8sMzMrJy0NFl0lXQg8CngoSLGY2ZmZailyWIWSVPjL0TEkvQZE6sypjEzs71ES09w/wT4Sd7wGuATxQrKzMzKS0ub+xgq6UFJr0l6VdIDkoYWOzgzs6accMIJu91gd/vtt/NP//RPzU7Xp08fANatW8fZZ5/d5LyzLsW//fbbG9wc99GPfrRD2m264YYbuOWWW9o9n47W0sNQc4GFwEHAwcDP0zIzs5KYPn06CxYsaFC2YMECpk+f3qLpDzroIO6///42L79xsnj44YcZMGDvbTKvpcliSETMjYiatPshMKSIcZmZNevss8/moYceYvv27QCsXbuWdevWcdxxx9Xf9zB+/HiOOuoofvazn+02/dq1axk9ejQA27ZtY9q0aYwZM4ZzzjmHbdu21de77LLL6ps3/9rXvgbA7NmzWbduHSeeeCInnngiAMOHD+f1118H4NZbb2X06NGMHj26vnnztWvXcuSRR/KP//iPjBo1ilNPPbXBcgp59tlnmThxImPGjOHjH/84b775Zv3yR44cyZgxY+obMPztb39b//CncePGsXnz5jav20JaetfI65LOB+5Nh6cDGzo0EjPbY33hC9DRD4AbOxbS7WxBgwYNYsKECTz66KNMnTqVBQsWcM455yCJnj178uCDD9KvXz9ef/11Jk6cyJQpU5p8tOgdd9xBr169WLZsGcuWLWvQxPiNN97I/vvvT21tLSeffDLLli3jyiuv5NZbb2Xx4sUMHjy4wbyWLl3K3Llzeeqpp4gIPvjBDzJp0iQGDhzIqlWruPfee/n+97/Ppz71KR544IFmn0/xmc98hu9+97tMmjSJr371q3z961/n9ttv56abbuLFF1+kR48e9Ye+brnlFubMmcOxxx7Lli1b6NmzZyvWdraW7ll8luSy2b8B64GzgYs6NBIzs1bKPxSVfwgqIrj22msZM2YMp5xyCn/961959dVXm5zP448/Xr/RHjNmDGPGjKkfd9999zF+/HjGjRvH8uXLMxsJfOKJJ/j4xz9O79696dOnD2eddRa/+93vABgxYgRjx44Fmm8GHZLna2zcuJFJkyYBcMEFF/D444/Xx3jeeecxb968+jvFjz32WK6++mpmz57Nxo0bO/wO8pZeDfUyyR3c9SR9AWgm74OkycB3gArgB42bCJF0KXA5UAtsAWakTYsg6Z+Bi9NxV0ZExzcVaWYdork9gGI688wzufrqq3nmmWfYtm1b/R7B/Pnzqa6uZunSpXTr1o3hw4cXbJY8X6G9jhdffJFbbrmFJUuWMHDgQC688MLM+TTX3l5d8+aQNHGedRiqKb/4xS94/PHHWbhwId/4xjdYvnw5M2fO5PTTT+fhhx9m4sSJ/OpXv+L9739/m+ZfSHseq9psUx+SKoA5wGnASGB62iBhvnsi4qiIGAvcDNyaTjsSmAaMAiYD/5bOz8ysXp8+fTjhhBP47Gc/2+DE9qZNm3jPe95Dt27dWLx4MS+99FKz8zn++OOZnz7j9c9//jPLli0DkubNe/fuTf/+/Xn11Vd55JFH6qfp27dvwfMCxx9/PD/96U/ZunUrb7/9Ng8++CAf/vCHW/3e+vfvz8CBA+v3Sn784x8zadIkdu7cySuvvMKJJ57IzTffzMaNG9myZQsvvPACRx11FNdccw25XI6//OUvrV5mc9qzn1L44N8uE4DV6T0ZSFoATAXq9+HSJkTq9AbqUvJUYEFEbAdelLQ6nd8f2hGvme2Fpk+fzllnndXgyqjzzjuPM844g1wux9ixYzP/YV922WVcdNFFjBkzhrFjxzJhwgQgeerduHHjGDVq1G7Nm8+YMYPTTjuNAw88kMWLF9eXjx8/ngsvvLB+Hpdccgnjxo1r9pBTU370ox9x6aWXsnXrVg477DDmzp1LbW0t559/Pps2bSIi+OIXv8iAAQP4yle+wuLFi6moqGDkyJH1T/3rKC1qorzghNLLEdHkc64knQ1MjohL0uFPAx+MiCsa1bucZC+lO3BSRKyS9K/AkxExL63z78AjEXF/o2lnADMAhg0bdkzWvwcz6zhuonzP054myps9DCVps6S3CnSbSe65aHbyAmW7ZaaImBMRhwPXANe3ctq7IiIXEbkhQ3wlr5lZsTR7GCoi2vM4pirgkLzhocC6ZuovAO5o47RmZlZE7TnBnWUJcISkEZK6k5ywXphfQdIReYOns6txwoXANEk90qfyHQE8XcRYzcysGUV7lFNE1Ei6gqS12grg7ohYLmkWUBkRC4ErJJ0CvAu8SfKMb9J695GcDK8BLo+I2mLFamZtU/esaCt/bT0/XafNJ7jLjZ/Bbda5XnzxRfr27cugQYOcMMpcRLBhwwY2b97MiBEjGozr0Gdwm5k1NnToUKqqqqiuri51KNYCPXv2ZOjQtjcW7mRhZm3SrVu33f6l2t6rmCe4zcxsL+FkYWZmmZwszMwsk5OFmZllcrIwM7NMThZmZpbJycLMzDI5WZiZWSYnCzMzy+RkYWZmmZwszMwsk5OFmZllcrIwM7NMThZmZpbJycLMzDI5WZiZWSYnCzMzy+RkYWZmmZwszMwsU1GThaTJklZKWi1pZoHxV0taIWmZpF9LOjRvXK2kZ9NuYTHjNDOz5nUt1owlVQBzgI8AVcASSQsjYkVetT8CuYjYKuky4GbgnHTctogYW6z4zMys5Yq5ZzEBWB0RayJiB7AAmJpfISIWR8TWdPBJYGgR4zEzszYqZrI4GHglb7gqLWvKxcAjecM9JVVKelLSmYUmkDQjrVNZXV3d/ojNzKygoh2GAlSgLApWlM4HcsCkvOJhEbFO0mHAY5Kei4gXGsws4i7gLoBcLldw3mZm1n7F3LOoAg7JGx4KrGtcSdIpwHXAlIjYXlceEevS1zXAb4BxRYzVzMyaUcxksQQ4QtIISd2BaUCDq5okjQPuJEkUr+WVD5TUI+0fDBwL5J8YNzOzTlS0w1ARUSPpCmARUAHcHRHLJc0CKiNiIfBtoA/wE0kAL0fEFOBI4E5JO0kS2k2NrqIyM7NOpIi941B/LpeLysrKUodhZrZHkbQ0InJZ9XwHt5mZZXKyMDOzTE4WZmaWycnCzMwyOVmYmVkmJwszM8vkZGFmZpmcLMzMLJOThZmZZXKyMDOzTE4WZmaWycnCzMwyOVmYmVkmJwszM8vkZGFmZpmcLMzMLJOThZmZZXKyMDOzTE4WZmaWycnCzMwyOVmYmVmmoiYLSZMlrZS0WtLMAuOvlrRC0jJJv5Z0aN64CyStSrsLihmnmZk1r2jJQlIFMAc4DRgJTJc0slG1PwK5iBgD3A/cnE67P/A14IPABOBrkgYWK1YzM2teMfcsJgCrI2JNROwAFgBT8ytExOKI2JoOPgkMTfv/AfhlRLwREW8CvwQmFzFWMzNrRjGTxcHAK3nDVWlZUy4GHmnNtJJmSKqUVFldXd3OcM3MrCnFTBYqUBYFK0rnAzng262ZNiLuiohcROSGDBnS5kDNzKx5xUwWVcAhecNDgXWNK0k6BbgOmBIR21szrZmZdY5iJoslwBGSRkjqDkwDFuZXkDQOuJMkUbyWN2oRcKqkgemJ7VPTMjMzK4GuxZpxRNRIuoJkI18B3B0RyyXNAiojYiHJYac+wE8kAbwcEVMi4g1J3yBJOACzIuKNYsVqZmbNU0TB0wh7nFwuF5WVlaUOw8xsjyJpaUTksur5Dm4zM8vkZGFmZpmcLMzMLJOThZmZZXKyMDOzTE4WZmaWycnCzMwyOVmYmVkmJwszM8vkZGFmZpmcLMzMLJOThZmZZXKyMDOzTE4WZmaWycnCzMwyOVmYmVkmJwszM8vkZGFmZpmK9gxuM7O9zY4dsGULbN6cvNbWQv/+MGAA9O0LXfbiv99OFhl27oRt25LunXd29TfXtbXejh3Qsyf06tW2br/9ssdXVHTu+quthXff3b3bsaNwef64nTuTeUi7Xlvb39bp8vu7d0/WX+/eu16Vvp1gAAAH4klEQVR79NhVx8pTBGzdumvDvnlzw/62vO7Y0fTyJOjXL0kc+V1dMskq69cPupbxFrmMQ+scGzbAmWc2vUFv7suRpVu3ZANdqOvdGwYPTpJDXVn37rB9e/IFz+9ee233sq1bd21MW6NHj+yk06NHyzfuWRv9iLavv3LWpUvDdZafSAq9tqVOSxN7BNTUJOt7+/bktS1dS6etU5dUOyopN9ffeHjnzmTj3dyGfcuWln//evSAPn2SvYO617594aCDdi/Pf+3aFTZtgo0bky6/f+NGWLt2V/mmTdnx9OnT+iTTvz8MGpR0xVTUZCFpMvAdoAL4QUTc1Gj88cDtwBhgWkTcnzeuFnguHXw5IqYUI8Zu3ZKuX7/dN+j5G/KWdvnTFPNffETywy2URNrabdiQvG7fvmu9dO++q78u+fXrV3hcftcR4yoqdv24Ilrf39bpGvfXree33y782rjs1Vd3r7NtW+s/4+7dGyaSulia24B3pIqKZCPavfuurlu3ZGNdt246cj23pL9uWEo2rPkb7gMPbHrD3tTGvu61W7firMN8O3cmiaypxFKofP16eP75XWW1tYXn/YEPwNNPFzf+oiULSRXAHOAjQBWwRNLCiFiRV+1l4ELgywVmsS0ixhYrvjr9+sFjjxV7KR1PSn7IPXrAwIGljsay1B3OzEo0zb126dJww13XNd6gN9e1tG5dsraO06VLshfQvz8cemjrp49IvguFkkqfPh0fb2PF3LOYAKyOiDUAkhYAU4H6ZBERa9NxbTigYrbn6NIl2Uvo3bvUkdieKn9vaujQzl9+Mc/dHwy8kjdclZa1VE9JlZKelHRmoQqSZqR1Kqurq9sTq5mZNaOYyaLQtSKtOd05LCJywLnA7ZIO321mEXdFRC4ickOGDGlrnGZmlqGYyaIKOCRveCiwrqUTR8S69HUN8BtgXEcGZ2ZmLVfMZLEEOELSCEndgWnAwpZMKGmgpB5p/2DgWPLOdZiZWecqWrKIiBrgCmAR8DxwX0QslzRL0hQASR+QVAV8ErhT0vJ08iOBSkl/AhYDNzW6isrMzDqRYi+5ayqXy0VlZWWpwzAz26NIWpqeH27WXtySiZmZdRQnCzMzy7TXHIaSVA28VOo42mkw8HqpgygjXh8NeX3s4nXRUHvWx6ERkXnvwV6TLPYGkipbcuxwX+H10ZDXxy5eFw11xvrwYSgzM8vkZGFmZpmcLMrLXaUOoMx4fTTk9bGL10VDRV8fPmdhZmaZvGdhZmaZnCzMzCyTk0UZkHSIpMWSnpe0XNJVpY6p1CRVSPqjpIdKHUupSRog6X5Jf0m/Ix8qdUylJOmL6e/kz5LuldSz1DF1Jkl3S3pN0p/zyvaX9EtJq9LXDn9+ppNFeagBvhQRRwITgcsljSxxTKV2FUkDlJY8x/7RiHg/cDT78HqRdDBwJZCLiNFABUmL1vuSHwKTG5XNBH4dEUcAv06HO5STRRmIiPUR8Uzav5lkY9CapwruVSQNBU4HflDqWEpNUj/geODfASJiR0RsLG1UJdcV2E9SV6AXrXhOzt4gIh4H3mhUPBX4Udr/I6Dg00Xbw8mizEgaTvKgp6dKG0lJ3Q78H8DPZofDgGpgbnpY7geS9tkneUfEX4FbgJeB9cCmiPjv0kZVFv4uItZD8ucTeE9HL8DJooxI6gM8AHwhIt4qdTylIOljwGsRsbTUsZSJrsB44I6IGAe8TREOMewp0mPxU4ERwEFAb0nnlzaqfYOTRZmQ1I0kUcyPiP8qdTwldCwwRdJaYAFwkqR5pQ2ppKqAqoio29O8nyR57KtOAV6MiOqIeBf4L+B/lTimcvCqpAMB0tfXOnoBThZlQJJIjkk/HxG3ljqeUoqIf46IoRExnOTE5WMRsc/+c4yIvwGvSHpfWnQy+/Yjhl8GJkrqlf5uTmYfPuGfZyFwQdp/AfCzjl5A146eobXJscCngeckPZuWXRsRD5cwJisfnwfmp8+yXwNcVOJ4SiYinpJ0P/AMyVWEf2Qfa/pD0r3ACcDg9LHUXwNuAu6TdDFJQv1khy/XzX2YmVkWH4YyM7NMThZmZpbJycLMzDI5WZiZWSYnCzMzy+RkYZZBUq2kZ/O6DruDWtLw/NZDzcqV77Mwy7YtIsaWOgizUvKehVkbSVor6VuSnk6796blh0r6taRl6euwtPzvJD0o6U9pV9dMRYWk76fPaPhvSful9a+UtCKdz4ISvU0zwMnCrCX2a3QY6py8cW9FxATgX0layyXt/4+IGAPMB2an5bOB30bE0STtOy1Py48A5kTEKGAj8Im0fCYwLp3PpcV6c2Yt4Tu4zTJI2hIRfQqUrwVOiog1aUOQf4uIQZJeBw6MiHfT8vURMVhSNTA0IrbnzWM48Mv0oTVIugboFhHflPQosAX4KfDTiNhS5Ldq1iTvWZi1TzTR31SdQrbn9dey61zi6cAc4BhgafqwH7OScLIwa59z8l7/kPb/nl2P+jwPeCLt/zVwGdQ/Y7xfUzOV1AU4JCIWkzwIagCw296NWWfxPxWzbPvltQYMyfOw6y6f7SHpKZI/XtPTsiuBuyX9b5Kn3NW1EnsVcFfaMmgtSeJY38QyK4B5kvoDAm7z41StlHzOwqyN0nMWuYh4vdSxmBWbD0OZmVkm71mYmVkm71mYmVkmJwszM8vkZGFmZpmcLMzMLJOThZmZZfr/jOpzhdL1s90AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network=models.Sequential()\n",
    "network.add(layers.Dense(units=2, activation='relu', input_shape=(4,) ))\n",
    "network.add(layers.Dense(units=2, activation ='relu'))\n",
    "network.add(layers.Dense(units=4, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer ='rmsprop', metrics=['accuracy'])\n",
    "X_val=X_train[:100]\n",
    "partial_X_train=X_train[100:]\n",
    "Y_val=Y_train_encoded[:100]\n",
    "partial_Y_train=Y_train_encoded[100:]\n",
    "history=model.fit(partial_X_train, partial_Y_train, epochs=10, batch_size=200, validation_data=(X_val, Y_val))\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1] # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuhuhalder/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13080/13080 [==============================] - 5s 399us/step - loss: 1.4693\n",
      "Epoch 2/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 1.2452\n",
      "Epoch 3/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 1.0643\n",
      "Epoch 4/200\n",
      "13080/13080 [==============================] - 0s 37us/step - loss: 0.9720\n",
      "Epoch 5/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.9200\n",
      "Epoch 6/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.8810\n",
      "Epoch 7/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.8493\n",
      "Epoch 8/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.8365\n",
      "Epoch 9/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.8108\n",
      "Epoch 10/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.7995\n",
      "Epoch 11/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.7851\n",
      "Epoch 12/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7708\n",
      "Epoch 13/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.7510\n",
      "Epoch 14/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7410\n",
      "Epoch 15/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.7362\n",
      "Epoch 16/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7344\n",
      "Epoch 17/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.7233\n",
      "Epoch 18/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7196\n",
      "Epoch 19/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7107\n",
      "Epoch 20/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7144\n",
      "Epoch 21/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7132\n",
      "Epoch 22/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.7046\n",
      "Epoch 23/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.7058\n",
      "Epoch 24/200\n",
      "13080/13080 [==============================] - 0s 37us/step - loss: 0.7017\n",
      "Epoch 25/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6975\n",
      "Epoch 26/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6997\n",
      "Epoch 27/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6980\n",
      "Epoch 28/200\n",
      "13080/13080 [==============================] - 0s 37us/step - loss: 0.7025\n",
      "Epoch 29/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6955\n",
      "Epoch 30/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6907\n",
      "Epoch 31/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6900\n",
      "Epoch 32/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6950\n",
      "Epoch 33/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6940\n",
      "Epoch 34/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6863\n",
      "Epoch 35/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6921\n",
      "Epoch 36/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6856\n",
      "Epoch 37/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6833\n",
      "Epoch 38/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6839\n",
      "Epoch 39/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6848\n",
      "Epoch 40/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6903\n",
      "Epoch 41/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6817\n",
      "Epoch 42/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6867\n",
      "Epoch 43/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6865\n",
      "Epoch 44/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6856\n",
      "Epoch 45/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6788\n",
      "Epoch 46/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6774\n",
      "Epoch 47/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6811\n",
      "Epoch 48/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6803\n",
      "Epoch 49/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6822\n",
      "Epoch 50/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6825\n",
      "Epoch 51/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6729\n",
      "Epoch 52/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6734\n",
      "Epoch 53/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6781\n",
      "Epoch 54/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6793\n",
      "Epoch 55/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6836\n",
      "Epoch 56/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6793\n",
      "Epoch 57/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6735\n",
      "Epoch 58/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6800\n",
      "Epoch 59/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6739\n",
      "Epoch 60/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6699\n",
      "Epoch 61/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6811\n",
      "Epoch 62/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6764\n",
      "Epoch 63/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6771\n",
      "Epoch 64/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6703\n",
      "Epoch 65/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6744\n",
      "Epoch 66/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6808\n",
      "Epoch 67/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6767\n",
      "Epoch 68/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6758\n",
      "Epoch 69/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6718\n",
      "Epoch 70/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6774\n",
      "Epoch 71/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6830\n",
      "Epoch 72/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6735\n",
      "Epoch 73/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6746\n",
      "Epoch 74/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6828\n",
      "Epoch 75/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6731\n",
      "Epoch 76/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6767\n",
      "Epoch 77/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6763\n",
      "Epoch 78/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6791\n",
      "Epoch 79/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6771\n",
      "Epoch 80/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6761\n",
      "Epoch 81/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6739\n",
      "Epoch 82/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6821\n",
      "Epoch 83/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6761\n",
      "Epoch 84/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6764\n",
      "Epoch 85/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6731\n",
      "Epoch 86/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6774\n",
      "Epoch 87/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6769\n",
      "Epoch 88/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6743\n",
      "Epoch 89/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6712\n",
      "Epoch 90/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6727\n",
      "Epoch 91/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6733\n",
      "Epoch 92/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6808\n",
      "Epoch 93/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6769\n",
      "Epoch 94/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6767\n",
      "Epoch 95/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6725\n",
      "Epoch 96/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6737\n",
      "Epoch 97/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6704\n",
      "Epoch 98/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6709\n",
      "Epoch 99/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6754\n",
      "Epoch 100/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6747\n",
      "Epoch 101/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6787\n",
      "Epoch 102/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6742\n",
      "Epoch 103/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6740\n",
      "Epoch 104/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6683\n",
      "Epoch 105/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6774\n",
      "Epoch 106/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6717\n",
      "Epoch 107/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6817\n",
      "Epoch 108/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6789\n",
      "Epoch 109/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6747\n",
      "Epoch 110/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6761\n",
      "Epoch 111/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6775\n",
      "Epoch 112/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6709\n",
      "Epoch 113/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6733\n",
      "Epoch 114/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6738\n",
      "Epoch 115/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6764\n",
      "Epoch 116/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6728\n",
      "Epoch 117/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6780\n",
      "Epoch 118/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6732\n",
      "Epoch 119/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6755\n",
      "Epoch 120/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6754\n",
      "Epoch 121/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6734\n",
      "Epoch 122/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6714\n",
      "Epoch 123/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6781\n",
      "Epoch 124/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6711\n",
      "Epoch 125/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6786\n",
      "Epoch 126/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6745\n",
      "Epoch 127/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6752\n",
      "Epoch 128/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6771\n",
      "Epoch 129/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6715\n",
      "Epoch 130/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6767\n",
      "Epoch 131/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6761\n",
      "Epoch 132/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6705\n",
      "Epoch 133/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6694\n",
      "Epoch 134/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6746\n",
      "Epoch 135/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6729\n",
      "Epoch 136/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6765\n",
      "Epoch 137/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6723\n",
      "Epoch 138/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6758\n",
      "Epoch 139/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6804\n",
      "Epoch 140/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6759\n",
      "Epoch 141/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6717\n",
      "Epoch 142/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6727\n",
      "Epoch 143/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6771\n",
      "Epoch 144/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6733\n",
      "Epoch 145/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6761\n",
      "Epoch 146/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6777\n",
      "Epoch 147/200\n",
      "13080/13080 [==============================] - 0s 36us/step - loss: 0.6753\n",
      "Epoch 148/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6731\n",
      "Epoch 149/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6809\n",
      "Epoch 150/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6793\n",
      "Epoch 151/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6741\n",
      "Epoch 152/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6703\n",
      "Epoch 153/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6774\n",
      "Epoch 154/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6812\n",
      "Epoch 155/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6751\n",
      "Epoch 156/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6753\n",
      "Epoch 157/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6765\n",
      "Epoch 158/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6734\n",
      "Epoch 159/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6750\n",
      "Epoch 160/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6817\n",
      "Epoch 161/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6727\n",
      "Epoch 162/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6770\n",
      "Epoch 163/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6810\n",
      "Epoch 164/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6794\n",
      "Epoch 165/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6688\n",
      "Epoch 166/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6739\n",
      "Epoch 167/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6704\n",
      "Epoch 168/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6667\n",
      "Epoch 169/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6744\n",
      "Epoch 170/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6754\n",
      "Epoch 171/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6727\n",
      "Epoch 172/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6751\n",
      "Epoch 173/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6762\n",
      "Epoch 174/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6746\n",
      "Epoch 175/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6801\n",
      "Epoch 176/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6722\n",
      "Epoch 177/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6725\n",
      "Epoch 178/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6699\n",
      "Epoch 179/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6763\n",
      "Epoch 180/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6753\n",
      "Epoch 181/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6720\n",
      "Epoch 182/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6746\n",
      "Epoch 183/200\n",
      "13080/13080 [==============================] - 0s 35us/step - loss: 0.6742\n",
      "Epoch 184/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6740\n",
      "Epoch 185/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6714\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6738\n",
      "Epoch 187/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6788\n",
      "Epoch 188/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6757\n",
      "Epoch 189/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6713\n",
      "Epoch 190/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6810\n",
      "Epoch 191/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6717\n",
      "Epoch 192/200\n",
      "13080/13080 [==============================] - 0s 34us/step - loss: 0.6703\n",
      "Epoch 193/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6697\n",
      "Epoch 194/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6648\n",
      "Epoch 195/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6707\n",
      "Epoch 196/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6671\n",
      "Epoch 197/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6714\n",
      "Epoch 198/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6751\n",
      "Epoch 199/200\n",
      "13080/13080 [==============================] - 0s 33us/step - loss: 0.6740\n",
      "Epoch 200/200\n",
      "13080/13080 [==============================] - 0s 32us/step - loss: 0.6765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3aa617b8>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nb_classes=4\n",
    "batch_size=128\n",
    "nb_epochs=200\n",
    "model=Sequential()\n",
    "model.add(Dense(3, input_dim=4))#input_shape\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) #?????\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# Model Training\n",
    "model.fit(X_train, Y_train_encoded , batch_size=batch_size, nb_epoch=nb_epochs,verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Neural Network  - Train accuracy: 0.0\n",
      "\n",
      "Deep Neural Network - Train Confusion Matrix\n",
      "\n",
      "Predicted     1     3\n",
      "Actuall              \n",
      "HTTP       1158     1\n",
      "PERF       7986     0\n",
      "PING         54     0\n",
      "VIDEO         0  3881\n",
      "\n",
      "Deep Neural Network  - Test accuracy: 0.0\n",
      "\n",
      "Deep Neural Network - Test Confusion Matrix\n",
      "\n",
      "Predicted     1     3\n",
      "Actuall              \n",
      "HTTP        511     1\n",
      "PERF       3425     0\n",
      "PING         20     0\n",
      "VIDEO         0  1649\n"
     ]
    }
   ],
   "source": [
    "#Model Prediction\n",
    "y_train_predclass = model.predict_classes(X_train,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(X_test,batch_size=batch_size)\n",
    "\n",
    "print (\"\\n\\nDeep Neural Network  - Train accuracy:\",(round(accuracy_score(Y_train,y_train_predclass),3))\n",
    "      )\n",
    "#print (\"\\nDeep Neural Network  - Train Classification Report\")\n",
    "#print (classification_report(Y_train,y_train_predclass))\n",
    "\n",
    "print (\"\\nDeep Neural Network - Train Confusion Matrix\\n\")\n",
    "print (pd.crosstab(Y_train ,y_train_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )  \n",
    "\n",
    "\n",
    "print (\"\\nDeep Neural Network  - Test accuracy:\",(round(accuracy_score(Y_test,y_test_predclass),3)))\n",
    "\n",
    "#print (\"\\nDeep Neural Network  - Test Classification Report\")\n",
    "#print (classification_report(Y_test,y_test_predclass))\n",
    "\n",
    "print (\"\\nDeep Neural Network - Test Confusion Matrix\\n\")\n",
    "print (pd.crosstab(Y_test,y_test_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
